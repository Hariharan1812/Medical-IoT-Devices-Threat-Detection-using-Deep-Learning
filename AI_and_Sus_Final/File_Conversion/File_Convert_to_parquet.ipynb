{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiDAIqSYxGWwqjKIKkPVfo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Data Cleaning and file Convertion**\n","\n","*   The \"CIC IoMT dataset 2024\" dataset is fetched from the source [link](https://www.unb.ca/cic/datasets/iomt-dataset-2024.html))\n","*   The dataset used is a open source dataset\n","\n","*   Note that no need to run every time and also for this course work I have aleary done this and saved the .parquet file at goolgle drive, so we can use that."],"metadata":{"id":"GFi_2I5QSbeR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngdXFom80Jqn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743089833587,"user_tz":0,"elapsed":64116,"user":{"displayName":"Hariharan Natesanpillai Ramalingam","userId":"15392509997735262428"}},"outputId":"26a8c53a-fbb9-4ff2-c287-8caa1b87a0ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","ğŸ’¾ Available RAM: 13.61 GB\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/ARP_Spoofing_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/MQTT-DoS-Connect_Flood_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/MQTT-Malformed_Data_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/MQTT-DDoS-Publish_Flood_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/Recon-OS_Scan_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/Recon-Ping_Sweep_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/MQTT-DoS-Publish_Flood_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/Recon-VulScan_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/MQTT-DDoS-Connect_Flood_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/Recon-Port_Scan_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/Benign_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DDoS-ICMP2_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DDoS-SYN_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DDoS-TCP_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DDoS-ICMP1_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DDoS-UDP1_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DoS-ICMP_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DDoS-UDP2_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DoS-TCP_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DoS-SYN_test.pcap.parquet\n","ğŸ—œï¸ Converted to Parquet: /content/drive/MyDrive/AI_and_sustainability/test/TCP_IP-DoS-UDP_test.pcap.parquet\n","âš ï¸ TPU is not available. Using CPU.\n","ğŸ§¹ Memory cleaned!\n"]}],"source":["import os\n","import sys\n","import gc\n","import gzip\n","import shutil\n","import glob\n","import pandas as pd\n","import psutil\n","import tensorflow as tf\n","from google.colab import drive\n","\n","dataset_path = \"<Enther the name of the dataset to be converted>\"\n","\n","# Define dataset directory\n","os.makedirs(dataset_path, exist_ok=True)  # Ensure directory exists\n","\n","# Delete unnecessary files (e.g., logs, metadata)\n","def clean_dataset(directory, extensions=[\".txt\", \".log\"]):\n","    for file in os.listdir(directory):\n","        if any(file.endswith(ext) for ext in extensions):\n","            os.remove(os.path.join(directory, file))\n","            print(f\"Deleted: {file}\")\n","\n","clean_dataset(dataset_path)\n","\n","# Convert PCAP to GZIP (compressed format)\n","def compress_pcap(file_path):\n","    compressed_path = file_path + \".gz\"\n","    with open(file_path, 'rb') as f_in:\n","        with gzip.open(compressed_path, 'wb') as f_out:\n","            shutil.copyfileobj(f_in, f_out)\n","    print(f\"Compressed: {compressed_path}\")\n","\n","# Compress all PCAP files in dataset\n","for pcap_file in glob.glob(dataset_path + \"/*.pcap\"):\n","    compress_pcap(pcap_file)\n","\n","# Convert CSV to Parquet (faster & smaller)\n","def convert_csv_to_parquet(file_path):\n","    df = pd.read_csv(file_path)\n","    parquet_path = file_path.replace(\".csv\", \".parquet\")\n","    df.to_parquet(parquet_path, engine=\"pyarrow\")\n","    print(f\"ğŸ—œï¸ Converted to Parquet: {parquet_path}\")\n","\n","for csv_file in glob.glob(dataset_path + \"/*.csv\"):\n","    convert_csv_to_parquet(csv_file)\n","\n","\n","# Free up memory\n","gc.collect()\n","print(\"Memory cleaned!\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Y9extEeOgEkj"},"execution_count":null,"outputs":[]}]}